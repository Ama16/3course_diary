# Информация о кодировании
| Title | Link | Key ideas |
|----------------|:---------:|----------------:|
| All about Categorical Variable Encoding | https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02 | Описание принципов работы One Hot, Label, Ordinal, Helmert, Binary, Frequency, Mean, Weight of Evidence, Probability Ratio, Hashing, Backward Difference, Leave One Out, James-Stein, M-estimator Encodings. |
| RealFeature encoding | https://istina.msu.ru/publications/article/7537819/ стр. 18 | Описание принципа работы RealFeature encoding. Признак со значением X кодируется при помощи статистики с другого признака от элементов обучающей выборки, значение кодируемого признака которых равны значению X. Применимо при наличии категориального и вещественного признаков. При факторных признаках может не показывать желаемого результата. |
| CountSvd Encoding | https://istina.msu.ru/publications/article/7537819/ стр. 19 | Описание принципа работы CountSvd encoding. Кодируемый признак кодируется k столбцами матрицы U в сингулярном разложении матрицы подсчета относительно кодируемого и кодирующего признаков. Применимо при наличии двую категориальных признаков|
| Target encoding | https://towardsdatascience.com/target-encoding-and-bayesian-target-encoding-5c6a6c58ae8c | Описание принципа работы Target encoding. Применяется при регрессионном или бинарном таргете (при многоклассовой классификации на n классов можно использовать в формате one vs all, получим n-1 новых признаков). Так как на преобразовании за счет таргета возможно переобучение, можно использовать регуляризацию (сглаживание).|
| Weight of Evidence Encoding | https://www.listendata.com/2015/03/weight-of-evidence-woe-and-information.html | Описание принципа работы Weight of Evidence Encoding. Работает только с бинарным таргетом (Можно сделать для работы не с категориальным, а с регрессионным признаком, разделив на категории. Категории с близнким значением WoE можно объединить.) Хорошо работает с логистической регрессией (тк упорядочивает категории в "логистической" шкале. Можно сравнивать зависимость от таргета для разных признаков, так как шкала одинаковая. Но так же мы можем переобучиться из-за зависимости от таргета. |
| James Stein Encoding | https://kiwidamien.github.io/james-stein-encoder.html | Описание принципа работы James Stein Encoding. Схожа с Target encoding. Заменяет значение i-ой категории по формуле: ![(1-B)p_i+Bp_{all}](https://latex.codecogs.com/svg.latex?&space;(1-B)p_i+Bp_{all}), где ![ B=\frac{var_i}{var_i + var_{all}}](https://latex.codecogs.com/svg.latex?&space;B=\frac{var_i}{var_i+var_{all}}). (![p_i](https://latex.codecogs.com/svg.latex?&space;p_i) - доля положительных примеров из i-ой категории, ![var_i](https://latex.codecogs.com/svg.latex?&space;var_i) - дисперсия i-го класса). Данное кодирование требует нормального распредленеия таргета (очевидно, что при бинарном таргете это не так: таргет лежит в отрезек [0;1]) Однако даже при такой "несостыковке" работает хорошо. |
|CatBoost: unbiased boosting with categorical features|https://arxiv.org/pdf/1706.09516.pdf|Обычный градиентный бустинг с кодировкой категориальный призноков с помощью статистик приводит к утечке таргета. Применяется упорядоченный бустинг. Генерируется случайная перестановка данных, и кодирование признака для i-го объекта происходит только с помощью первых (i-1) объектов, что позволяет избежать утечки таргета. Обучение i-ой модели в бустинге так же осуществляется с помощью только первых i объектов, оценка j-го осуществляется по (j-1) модели. |
|Encoding high-cardinality string categorical variables|https://arxiv.org/pdf/1907.01860.pdf (Min-hash encoding)|Min-hash функция для каждого значения строки определяется как: ![Z_j(\mathcal{X})\=](https://latex.codecogs.com/svg.latex?&space;Z_j(\mathcal{X})\=) ![\min_{x\in\mathcal{X}}\pi_j(x)](https://latex.codecogs.com/svg.latex?&space;\min_{x\in\mathcal{X}}\pi_j(x)), где  ![pi_j(x)](https://latex.codecogs.com/svg.latex?&space;\pi_j(x)) - j-ая хэш функция. Min-hash encoder определяется как ![x^{min-hash}(s)\=](https://latex.codecogs.com/svg.latex?&space;x^{min-hash}(s)\=)![[Z_1(\mathcal{G}(s))\,\ldots\,Z_d(\mathcal{G}(s))]](https://latex.codecogs.com/svg.latex?&space;[Z_1(\mathcal{G}(s))\,\ldots\,Z_d(\mathcal{G}(s))]), где ![\mathcal{G}\(s\)](https://latex.codecogs.com/svg.latex?&space;\mathcal{G}\(s\)) - набор последовательных n-грамм для строки s. Свойство: ![\frac{1}{d}\mathbb{E}[\|\|x^{min-hash}(si)-x^{min-hash}(sj)\|\|]\=J(\mathcal{G}(si)\,\mathcal{G}(sj))](https://latex.codecogs.com/svg.latex?&space;\frac{1}{d}\mathbb{E}[\|\|x^{min-hash}(si)-x^{min-hash}(sj)\|\|]\=J(\mathcal{G}(si)\,\mathcal{G}(sj))), где l0 норма. Т.e. мы можем быстро проаппроксимировать коэффициент Жаккара между 2 множествами n-грамм. Если строка  1 вложена в строку 2, то min-hash функция строки 1 не меньше min-hash функции строки 2. Значит, мы преобразовали отношения включения строк в отношение порядка в пространстве признаков (это достаточно важно для моделей с деревьями). Эта кодировка быстра для вычислений, можно кодировать распределенно. Так как используются хэши, трудно интерпретировать в терминах исходной строки|
