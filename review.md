# Информация о кодировании
(Latex картинки могут, по каким-то пирчинам, иногда не отображаться)
| Title | Link | Key ideas |
|----------------|:---------:|----------------:|
| All about Categorical Variable Encoding | https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02 | Описание принципов работы One Hot, Label, Ordinal, Helmert, Binary, Frequency, Mean, Weight of Evidence, Probability Ratio, Hashing, Backward Difference, Leave One Out, James-Stein, M-estimator Encodings. |
| RealFeature encoding | https://istina.msu.ru/publications/article/7537819/ стр. 18 | Описание принципа работы RealFeature encoding. Признак со значением X кодируется при помощи статистики с другого признака от элементов обучающей выборки, значение кодируемого признака которых равны значению X. Применимо при наличии категориального и вещественного признаков. При факторных признаках может не показывать желаемого результата. |
| CountSvd Encoding | https://istina.msu.ru/publications/article/7537819/ стр. 19 | Описание принципа работы CountSvd encoding. Кодируемый признак кодируется k столбцами матрицы U в сингулярном разложении матрицы подсчета относительно кодируемого и кодирующего признаков. Применимо при наличии двую категориальных признаков|
| Target encoding | https://towardsdatascience.com/target-encoding-and-bayesian-target-encoding-5c6a6c58ae8c | Описание принципа работы Target encoding. Применяется при регрессионном или бинарном таргете (при многоклассовой классификации на n классов можно использовать в формате one vs all, получим n-1 новых признаков). Так как на преобразовании за счет таргета возможно переобучение, можно использовать регуляризацию (сглаживание).|
| Weight of Evidence Encoding | https://www.listendata.com/2015/03/weight-of-evidence-woe-and-information.html | Описание принципа работы Weight of Evidence Encoding. Работает только с бинарным таргетом (Можно сделать для работы не с категориальным, а с регрессионным признаком, разделив на категории. Категории с близнким значением WoE можно объединить.) Хорошо работает с логистической регрессией (тк упорядочивает категории в "логистической" шкале. Можно сравнивать зависимость от таргета для разных признаков, так как шкала одинаковая. Но так же мы можем переобучиться из-за зависимости от таргета. |
| James Stein Encoding | https://kiwidamien.github.io/james-stein-encoder.html | Описание принципа работы James Stein Encoding. Схожа с Target encoding. Заменяет значение i-ой категории по формуле: ![(1-B)p_i+Bp_{all}](https://latex.codecogs.com/svg.latex?&space;(1-B)p_i+Bp_{all}), где ![ B=\frac{var_i}{var_i + var_{all}}](https://latex.codecogs.com/svg.latex?&space;B=\frac{var_i}{var_i+var_{all}}). (![p_i](https://latex.codecogs.com/svg.latex?&space;p_i) - доля положительных примеров из i-ой категории, ![var_i](https://latex.codecogs.com/svg.latex?&space;var_i) - дисперсия i-го класса). Данное кодирование требует нормального распредленеия таргета (очевидно, что при бинарном таргете это не так: таргет лежит в отрезек [0;1]) Однако даже при такой "несостыковке" работает хорошо. |
|CatBoost: unbiased boosting with categorical features|https://arxiv.org/pdf/1706.09516.pdf|Обычный градиентный бустинг с кодировкой категориальный призноков с помощью статистик приводит к утечке таргета. Применяется упорядоченный бустинг. Генерируется случайная перестановка данных, и кодирование признака для i-го объекта происходит только с помощью первых (i-1) объектов, что позволяет избежать утечки таргета. Обучение i-ой модели в бустинге так же осуществляется с помощью только первых i объектов, оценка j-го осуществляется по (j-1) модели. |
|Encoding high-cardinality string categorical variables|https://arxiv.org/pdf/1907.01860.pdf (Min-hash encoding)|Min-hash функция для каждого значения строки определяется как: ![Z_{j}(\mathcal{X})\=](https://latex.codecogs.com/svg.latex?&space;Z_{j}(\mathcal{X})\=) ![\min_{x\in\mathcal{X}}](https://latex.codecogs.com/svg.latex?&space;\min_{x\in\mathcal{X}}) ![\pi_j(x)](https://latex.codecogs.com/svg.latex?&space;\pi_j(x)), где  ![pi_j(x)](https://latex.codecogs.com/svg.latex?&space;\pi_j(x)) - j-ая хэш функция. Min-hash encoder определяется как ![x^{m}](https://latex.codecogs.com/svg.latex?&space;x^{m}) ![(s)\=](https://latex.codecogs.com/svg.latex?&space;(s)\=) ![[Z_1(\mathcal{G}(s))\,\ldots\,Z_d(\mathcal{G}(s))]](https://latex.codecogs.com/svg.latex?&space;[Z_1(\mathcal{G}(s))\,\ldots\,Z_d(\mathcal{G}(s))]), где ![\mathcal{G}](https://latex.codecogs.com/svg.latex?&space;\mathcal{G}) ![(s)](https://latex.codecogs.com/svg.latex?&space;(s)) - набор последовательных n-грамм для строки s. Свойство: ![\frac{1}{d}\mathbb{E}[\|\|x^{min-hash}(si)-x^{min-hash}(sj)\|\|]\=J(\mathcal{G}(si)\,\mathcal{G}(sj))](https://latex.codecogs.com/svg.latex?&space;\frac{1}{d}\mathbb{E}[\|\|x^{min-hash}(si)-x^{min-hash}(sj)\|\|]\=J(\mathcal{G}(si)\,\mathcal{G}(sj))), где l0 норма. Т.e. мы можем быстро проаппроксимировать коэффициент Жаккара между 2 множествами n-грамм. Если строка  1 вложена в строку 2, то min-hash функция строки 1 не меньше min-hash функции строки 2. Значит, мы преобразовали отношения включения строк в отношение порядка в пространстве признаков (это достаточно важно для моделей с деревьями). Эта кодировка быстра для вычислений, можно кодировать распределенно. Так как используются хэши, трудно интерпретировать в терминах исходной строки|
|Encoding high-cardinality string categorical variables|https://arxiv.org/pdf/1907.01860.pdf (Gamma-Poisson factorization)|Составляем матрицу подсчета n-грамм F для наших строк. Каждая строка моделируется как линейная комбинация d неизвестных нам тем. Для тренировочной подвыборки оцениваем матрицу ![\Lambda](https://latex.codecogs.com/svg.latex?&space;\Lambda)![\in](https://latex.codecogs.com/svg.latex?&space;\in)![\mathbb{R}^{d\times{}m}](https://latex.codecogs.com/svg.latex?&space;\mathbb{R}^{d\times{}m}), где m - количество всевозможных n-грамм в трейне. ![F\thickapprox{}](https://latex.codecogs.com/svg.latex?&space;F\thickapprox{}) ![X\Lambda](https://latex.codecogs.com/svg.latex?&space;X\Lambda), ![F\in{}\mathbb{R}^{n\times{}m}](https://latex.codecogs.com/svg.latex?&space;F\in{}\mathbb{R}^{n\times{}m}), ![X\in](https://latex.codecogs.com/svg.latex?&space;X\in) ![\mathbb{R}^{n\times{}d}](https://latex.codecogs.com/svg.latex?&space;\mathbb{R}^{n\times{}d}). Считаем, что ![f_{i}](https://latex.codecogs.com/svg.latex?&space;f_{i}) распределены по Пуассону, а ![x_{i}](https://latex.codecogs.com/svg.latex?&space;x_{i}) - Гамма распределение (отсюда и название). Для обучения максимизируем логарифм правдоподобия выборки. Рекуррентно ищем ![\Lambda_{i\,j}](https://latex.codecogs.com/svg.latex?&space;\Lambda_{i\,j}) и ![x_{i}](https://latex.codecogs.com/svg.latex?&space;x_{i}) (Формулы и псевдокод в статье). Можно интерптетировать полученные значения: можно отслеживать feature maps, соответствующие каждому слову (n-грамме), и назначать метки на основе входных категорий, которые наиболее активны в данной размерности. По экспериментам с разными датасетами (если верить статье), этот метод практически всегда сильно лучше других (TF-IDF + SVD, FastText + SVD, Bert + SVD) (Normalized Mutual Information (NMI) в качестве метрики способности кодировщика восстанавливать матрицу признаков, близкую к one-hot)| 
